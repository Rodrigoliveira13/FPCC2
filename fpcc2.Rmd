---
title: 'Reanálise De FPCC 2'
author: "Rodrigo Oliveira"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

\pagebreak

# Intro

Included in this R Markdown file are our stepwise regression analyses for predicting 1) Learning Rate, 2) Programming Accuracy, and 3) Declarative Knowledge (please see the corresponding readme.txt file in our github for detailed information on our dependent variables and our predictors). 

For our stepwise regressions, we utilized the package olsrr developed by Aravind Hebbali.

https://www.rdocumentation.org/packages/olsrr/versions/0.5.2

Specifically, the stepwise regression method we chose (ols_step_both_p) builds a "regression model from a set of candidate predictor variables by entering and removing predictors based on p values, in a stepwise manner until there is no variable left to enter or remove any more." Missing values are removed in a listwise manner prior to running the stepwise regression for every dependent variable. Only participants with an EEG Usability score of 1 or greater (indicating that they had at least one detectable alpha peak in the O2 channel) were utilized in analyses.

Finally, Bayesian Information Criterion (BIC) values are reported for every step of the regression analyses for every dependent variable, up to the final model output determined by the stepwise regression. BIC values are also reported for models that include an additional excluded variable on top of the variables included in the final model to allow for comparison with the final model determined by the stepwise regression.

```{r}
install.packages("olsrr")
```

```{r}
library(modelr)
library(broom)
```



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse, warn.conflicts=FALSE, quietly=TRUE)
library(knitr)
library(olsrr)
```

```{r}
dados <- read.csv("Computer_Whisperers_Stepwise_Regression_Data_for_pub.csv") %>%
  filter(EEG_Usability >= 1)
```

\pagebreak

# DV #1: Learning Rate

## Predictors:

* Fluid Intelligence
* Language Aptitude
* Numeracy
* Working Memory Updating
* Working Memory Span
* Right Fronto-Temporal Beta Power

```{r, include=FALSE}
learning_rate <- rawdata[, c("Learning_Rate", "Fluid_Intelligence", "Language_Aptitude", "Numeracy", "Working_Memory_Updating", "Working_Memory_Span", "Right_FrontoTemporal_Beta_Power")] %>% na.omit()
```

All six (6) predictors are entered into the stepwise regression. The final model consists of Fluid Intelligence, Right Fronto-Temporal Beta Power, and Numeracy.

```{r}
dados %>% 
  ggplot(aes(x = Learning_Rate, y = Fluid_Intelligence)) +
  geom_point(alpha = 1) + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Correlacionamento entre Taxa de Aprendizagem e Inteligência Fluída") + 
  xlab("Taxa de Aprendizagem") + 
  ylab("Inteligência Fluida")
```

```{r}
dados %>% 
  ggplot(aes(x = Learning_Rate, y = Language_Aptitude)) +
  geom_point(alpha = 1) + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Correlacionamento entre Taxa de Aprendizagem e Linguagem de Aptidão") + 
  xlab("Taxa de Aprendizagem") + 
  ylab("Linguagem de Aptidão")
```
```{r}
dados %>% 
  ggplot(aes(x = Learning_Rate, y = Right_FrontoTemporal_Low_Gamma_Power)) +
  geom_point(alpha = 1) + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Cor. entre Taxa de Apren. e Baixa Freq. Gama no Fronto Temp. Dir.") + 
  xlab("Taxa de Aprendizagem") + 
  ylab("Baixa Freqência Gama no Fronto Temporal Direito")
```
```{r}
dados %>% 
  ggplot(aes(x = Learning_Rate, y = Numeracy)) +
  geom_point(alpha = 1) + 
  geom_smooth(method = "lm", se = FALSE) +
  ggtitle("Correlação entre Taxa de Aprendizagem e Numeramento") + 
  xlab("Taxa de Aprendizagem") + 
  ylab("Numeramento")
```



```{r, echo=FALSE}
learning_rate_model <- lm(Learning_Rate ~ ., data = learning_rate)
learning_rate_model_summary <- ols_step_both_p(learning_rate_model, details = T)
```

## Obtaining Bayesian Information Criterion Values

m1 through m4 are models produced by each step of the stepwise regression, with m4 being the final model with the best BIC value. m5a (+ Working Memory Span) and m5b (+ Working Memory Updating) are models that include an additional variable on top of the variables included in the final model; they have worse BIC values compared to m4.

```{r, echo=FALSE}
m4_LR <- lm(Learning_Rate ~ Language_Aptitude + Fluid_Intelligence + Right_FrontoTemporal_Beta_Power + Numeracy, learning_rate)

tidy (m4_LR, conf.int = TRUE)
glance (m4_LR)
```


```{r}
tidy(m4_LR, conf.int = TRUE, exponentiate = TRUE) %>% 
  filter(term!= "(Intercept)") %>% 
  ggplot(aes(x = term, y = estimate, ymin = conf.low, ymax = conf.high)) + 
  geom_point() + 
  geom_linerange() + 
  ggtitle("IC das variáveis preditoras") + 
  coord_flip()
```


